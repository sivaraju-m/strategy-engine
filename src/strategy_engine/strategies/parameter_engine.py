"""
================================================================================
Strategy Parameter Engine
================================================================================

This module provides a unified interface for managing, optimizing, and tracking
parameters for trading strategies. It features:

- Parameter memory and auto-tuning
- Grid search and optimization utilities
- Performance tracking and analytics
- Integration with Firestore and BigQuery for persistence
- Real-time and batch parameter generation
- Backtesting support and summary statistics

Designed for integration into larger trading systems, it enables robust
parameter management and continuous improvement of trading strategies.

================================================================================
"""

import itertools
import json
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Optional

import numpy as np
from google.cloud import bigquery, firestore

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class StrategyParameters:
    """Strategy parameter configuration"""

    strategy_id: str
    parameters: dict[str, Any]
    parameter_hash: str
    created_at: datetime = None


@dataclass
class StrategyPerformance:
    """Strategy performance metrics"""

    strategy_id: str
    parameter_hash: str
    sharpe_ratio: float
    annual_return: float
    max_drawdown: float
    win_rate: float
    total_trades: int
    avg_trade_return: float
    volatility: float
    calmar_ratio: float
    sortino_ratio: float
    backtest_start: datetime
    backtest_end: datetime
    timestamp: datetime = None
    status: str = "completed"


@dataclass
class ParameterOptimizationResult:
    """Parameter optimization result"""

    strategy_id: str
    best_parameters: dict[str, Any]
    best_performance: StrategyPerformance
    optimization_history: list[StrategyPerformance]
    total_combinations_tested: int
    optimization_time_seconds: float
    timestamp: datetime = None


class StrategyParameterEngine:
    """
    Manages strategy parameters with memory and auto-optimization
    """

    def __init__(self, project_id: str):
        """Initialize the strategy parameter engine"""
        self.project_id = project_id

        # Initialize clients
        self.firestore_client = firestore.Client(project=project_id)
        self.bq_client = bigquery.Client(project=project_id)

        # Performance thresholds
        self.min_sharpe_ratio = 1.0
        self.max_drawdown_threshold = 0.20  # 20%
        self.min_win_rate = 0.45  # 45%

        # Optimization settings
        self.max_concurrent_backtests = 5
        self.optimization_timeout = 3600  # 1 hour

    def generate_parameter_combinations(
        self, strategy_id: str, parameter_grid: dict[str, list[Any]]
    ) -> list[StrategyParameters]:
        """
        Generate all parameter combinations for optimization

        Args:
            strategy_id: Strategy identifier
            parameter_grid: Dictionary of parameter names and their possible values

        Returns:
            List of parameter combinations
        """
        try:
            # Generate all combinations
            param_names = list(parameter_grid.keys())
            param_values = list(parameter_grid.values())

            combinations = []
            for combo in itertools.product(*param_values):
                param_dict = dict(zip(param_names, combo))
                param_hash = self._generate_parameter_hash(param_dict)

                # Check if this combination has been tested before
                if not self._is_combination_tested(strategy_id, param_hash):
                    combinations.append(
                        StrategyParameters(
                            strategy_id=strategy_id,
                            parameters=param_dict,
                            parameter_hash=param_hash,
                            created_at=datetime.now(),
                        )
                    )

            logger.info(
                "Generated {len(combinations)} new parameter combinations for {strategy_id}"
            )
            return combinations

        except Exception as e:
            logger.error("Error generating parameter combinations: {str(e)}")
            raise

    def _generate_parameter_hash(self, parameters: dict[str, Any]) -> str:
        """Generate unique hash for parameter combination"""
        import hashlib

        # Sort parameters for consistent hashing
        sorted_params = json.dumps(parameters, sort_keys=True)
        return hashlib.sha256(sorted_params.encode()).hexdigest()

    def _is_combination_tested(self, strategy_id: str, parameter_hash: str) -> bool:
        """Check if parameter combination has been tested before"""
        try:
            # Check in Firestore
            query = (
                self.firestore_client.collection("strategy_performance")
                .where("strategy_id", "==", strategy_id)
                .where("parameter_hash", "==", parameter_hash)
                .limit(1)
            )

            docs = list(query.stream())

            if docs:
                # Check if the test was successful (not failed)
                doc_data = docs[0].to_dict()
                return doc_data.get("status") == "completed"

            return False

        except Exception as e:
            logger.error("Error checking if combination tested: {str(e)}")
            return False

    def optimize_strategy_parameters(
        self,
        strategy_id: str,
        parameter_grid: dict[str, list[Any]],
        start_date: str,
        end_date: str,
        ticker: str = "NIFTY50",
    ) -> ParameterOptimizationResult:
        """
        Optimize strategy parameters using grid search

        Args:
            strategy_id: Strategy to optimize
            parameter_grid: Parameter search space
            start_date: Backtest start date
            end_date: Backtest end date
            ticker: Ticker symbol for backtesting

        Returns:
            Optimization results
        """
        start_time = datetime.now()

        try:
            # Generate parameter combinations
            parameter_combinations = self.generate_parameter_combinations(
                strategy_id, parameter_grid
            )

            if not parameter_combinations:
                logger.info("No new parameter combinations to test for {strategy_id}")
                return self._get_best_existing_performance(strategy_id)

            logger.info(
                "Starting optimization for {strategy_id} with {len(parameter_combinations)} combinations"
            )

            # Run backtests in parallel
            optimization_results = []

            with ThreadPoolExecutor(
                max_workers=self.max_concurrent_backtests
            ) as executor:
                # Submit all backtest jobs
                future_to_params = {
                    executor.submit(
                        self._run_single_backtest,
                        strategy_id,
                        params.parameters,
                        params.parameter_hash,
                        start_date,
                        end_date,
                        ticker,
                    ): params
                    for params in parameter_combinations
                }

                # Collect results as they complete
                for future in as_completed(
                    future_to_params, timeout=self.optimization_timeout
                ):
                    params = future_to_params[future]
                    try:
                        result = future.result()
                        if result:
                            optimization_results.append(result)
                            logger.info(
                                "Completed backtest for {params.parameter_hash}: Sharpe={result.sharpe_ratio:.2f}"
                            )
                    except Exception as e:
                        logger.error(
                            "Backtest failed for {params.parameter_hash}: {str(e)}"
                        )
                        # Store failed attempt
                        self._store_failed_attempt(
                            strategy_id, params.parameter_hash, str(e)
                        )

            # Find best performing parameters
            if not optimization_results:
                raise ValueError("No successful backtests completed")

            best_result = max(optimization_results, key=lambda x: x.sharpe_ratio)

            # Create optimization result
            optimization_time = (datetime.now() - start_time).total_seconds()

            optimization_result = ParameterOptimizationResult(
                strategy_id=strategy_id,
                best_parameters=(
                    best_result.parameters if hasattr(best_result, "parameters") else {}
                ),
                best_performance=best_result,
                optimization_history=optimization_results,
                total_combinations_tested=len(optimization_results),
                optimization_time_seconds=optimization_time,
                timestamp=datetime.now(),
            )

            # Store optimization result
            self._store_optimization_result(optimization_result)

            logger.info(
                "Optimization completed for {strategy_id}. Best Sharpe: {best_result.sharpe_ratio:.2f}"
            )

            return optimization_result

        except Exception as e:
            logger.error("Error optimizing strategy {strategy_id}: {str(e)}")
            raise

    def _run_single_backtest(
        self,
        strategy_id: str,
        parameters: dict[str, Any],
        parameter_hash: str,
        start_date: str,
        end_date: str,
        ticker: str,
    ) -> Optional[StrategyPerformance]:
        """
        Run a single backtest with given parameters

        This is a placeholder - in real implementation, this would call
        the actual backtest engine with the specific parameters
        """
        try:
            # Import the backtest engine
            pass

            # Simulate strategy-specific parameter application
            # In real implementation, this would configure the strategy with parameters
            # For demonstration, generate mock performance based on parameters
            # In production, this would run actual backtest
            # Mock performance calculation (replace with actual backtest)
            performance = self._generate_mock_performance(
                strategy_id, parameters, start_date, end_date
            )

            # Store performance
            self._store_strategy_performance(performance)

            return performance

        except Exception as e:
            logger.error("Error running backtest for {parameter_hash}: {str(e)}")
            return None

    def _generate_mock_performance(
        self,
        strategy_id: str,
        parameters: dict[str, Any],
        start_date: str,
        end_date: str,
    ) -> StrategyPerformance:
        """
        Generate mock performance for demonstration
        In production, this would be replaced with actual backtest results
        """
        # Generate performance based on parameter values
        # This is a simplified mock - real implementation would run actual backtests

        # Use parameter hash to ensure consistent results
        param_hash = self._generate_parameter_hash(parameters)

        # Simple performance simulation based on parameters
        np.random.seed(int(param_hash[:8], 16) % 2**32)  # Deterministic based on hash

        # Base performance metrics
        annual_return = np.random.normal(0.12, 0.08)  # 12% +/- 8%
        volatility = np.random.uniform(0.15, 0.35)  # 15-35% volatility
        sharpe_ratio = annual_return / volatility
        max_drawdown = np.random.uniform(0.05, 0.25)  # 5-25% drawdown
        win_rate = np.random.uniform(0.35, 0.65)  # 35-65% win rate

        # Adjust based on parameter quality (simplified)
        param_quality_score = self._calculate_parameter_quality(parameters)
        sharpe_ratio *= param_quality_score
        annual_return *= param_quality_score

        return StrategyPerformance(
            strategy_id=strategy_id,
            parameter_hash=param_hash,
            sharpe_ratio=sharpe_ratio,
            annual_return=annual_return,
            max_drawdown=max_drawdown,
            win_rate=win_rate,
            total_trades=np.random.randint(50, 200),
            avg_trade_return=annual_return / 100,
            volatility=volatility,
            calmar_ratio=annual_return / max_drawdown if max_drawdown > 0 else 0,
            sortino_ratio=sharpe_ratio * 1.2,  # Simplified
            backtest_start=datetime.strptime(start_date, "%Y-%m-%d"),
            backtest_end=datetime.strptime(end_date, "%Y-%m-%d"),
            timestamp=datetime.now(),
            status="completed",
        )

    def _calculate_parameter_quality(self, parameters: dict[str, Any]) -> float:
        """
        Calculate parameter quality score (simplified heuristic)
        In production, this would be based on domain knowledge
        """
        quality_score = 1.0

        # Example parameter quality rules
        if "lookback_period" in parameters:
            lookback = parameters["lookback_period"]
            if 10 <= lookback <= 50:
                quality_score *= 1.1  # Good lookback period
            elif lookback < 5 or lookback > 100:
                quality_score *= 0.8  # Poor lookback period

        if "threshold" in parameters:
            threshold = parameters["threshold"]
            if 0.01 <= threshold <= 0.05:
                quality_score *= 1.05  # Good threshold

        return min(quality_score, 1.3)  # Cap at 30% bonus

    def _store_strategy_performance(self, performance: StrategyPerformance):
        """Store strategy performance in Firestore and BigQuery"""
        try:
            # Store in Firestore
            performance_doc = asdict(performance)
            # Convert datetime objects to Firestore timestamp
            performance_doc["timestamp"] = firestore.SERVER_TIMESTAMP
            performance_doc["backtest_start"] = performance.backtest_start
            performance_doc["backtest_end"] = performance.backtest_end

            doc_ref = self.firestore_client.collection("strategy_performance").add(
                performance_doc
            )

            # Store in BigQuery for analytics
            self._log_performance_to_bigquery(performance)

            logger.debug(
                "Stored performance for {performance.strategy_id}:{performance.parameter_hash}"
            )

        except Exception as e:
            logger.error("Error storing strategy performance: {str(e)}")

    def _store_failed_attempt(
        self, strategy_id: str, parameter_hash: str, error_message: str
    ):
        """Store failed backtest attempt"""
        try:
            failed_doc = {
                "strategy_id": strategy_id,
                "parameter_hash": parameter_hash,
                "status": "failed",
                "error_message": error_message,
                "timestamp": firestore.SERVER_TIMESTAMP,
            }

            self.firestore_client.collection("strategy_performance").add(failed_doc)

        except Exception as e:
            logger.error("Error storing failed attempt: {str(e)}")

    def _store_optimization_result(self, result: ParameterOptimizationResult):
        """Store optimization result"""
        try:
            result_doc = {
                "strategy_id": result.strategy_id,
                "best_parameters": result.best_parameters,
                "best_sharpe_ratio": result.best_performance.sharpe_ratio,
                "best_annual_return": result.best_performance.annual_return,
                "combinations_tested": result.total_combinations_tested,
                "optimization_time_seconds": result.optimization_time_seconds,
                "timestamp": firestore.SERVER_TIMESTAMP,
            }

            self.firestore_client.collection("parameter_optimizations").add(result_doc)

            # Update strategy with best parameters
            self._update_strategy_best_parameters(
                result.strategy_id, result.best_parameters
            )

        except Exception as e:
            logger.error("Error storing optimization result: {str(e)}")

    def _update_strategy_best_parameters(
        self, strategy_id: str, best_parameters: dict[str, Any]
    ):
        """Update strategy with best performing parameters"""
        try:
            strategy_doc = {
                "strategy_id": strategy_id,
                "best_parameters": best_parameters,
                "last_optimization": firestore.SERVER_TIMESTAMP,
                "status": "optimized",
            }

            doc_ref = self.firestore_client.collection("strategies").document(
                strategy_id
            )
            doc_ref.set(strategy_doc, merge=True)

        except Exception as e:
            logger.error("Error updating strategy best parameters: {str(e)}")

    def _log_performance_to_bigquery(self, performance: StrategyPerformance):
        """Log performance to BigQuery"""
        try:
            table_id = "{self.project_id}.trading_analytics.strategy_performance"

            rows_to_insert = [
                {
                    "strategy_id": performance.strategy_id,
                    "parameter_hash": performance.parameter_hash,
                    "sharpe_ratio": performance.sharpe_ratio,
                    "annual_return": performance.annual_return,
                    "max_drawdown": performance.max_drawdown,
                    "win_rate": performance.win_rate,
                    "total_trades": performance.total_trades,
                    "avg_trade_return": performance.avg_trade_return,
                    "volatility": performance.volatility,
                    "calmar_ratio": performance.calmar_ratio,
                    "sortino_ratio": performance.sortino_ratio,
                    "backtest_start": performance.backtest_start.isoformat(),
                    "backtest_end": performance.backtest_end.isoformat(),
                    "timestamp": (
                        performance.timestamp.isoformat()
                        if performance.timestamp
                        else datetime.now().isoformat()
                    ),
                    "status": performance.status,
                }
            ]

            errors = self.bq_client.insert_rows_json(table_id, rows_to_insert)
            if errors:
                logger.error("BigQuery insert errors: {errors}")

        except Exception as e:
            logger.error("Error logging performance to BigQuery: {str(e)}")

    def _get_best_existing_performance(
        self, strategy_id: str
    ) -> ParameterOptimizationResult:
        """Get best existing performance for strategy"""
        try:
            # Query best performance from Firestore
            query = (
                self.firestore_client.collection("strategy_performance")
                .where("strategy_id", "==", strategy_id)
                .where("status", "==", "completed")
                .order_by("sharpe_ratio", direction=firestore.Query.DESCENDING)
                .limit(1)
            )

            docs = list(query.stream())

            if not docs:
                raise ValueError(
                    "No existing performance data for strategy {strategy_id}"
                )

            best_doc = docs[0].to_dict()

            # Convert to StrategyPerformance object
            best_performance = StrategyPerformance(
                strategy_id=best_doc["strategy_id"],
                parameter_hash=best_doc["parameter_hash"],
                sharpe_ratio=best_doc["sharpe_ratio"],
                annual_return=best_doc["annual_return"],
                max_drawdown=best_doc["max_drawdown"],
                win_rate=best_doc["win_rate"],
                total_trades=best_doc["total_trades"],
                avg_trade_return=best_doc["avg_trade_return"],
                volatility=best_doc["volatility"],
                calmar_ratio=best_doc["calmar_ratio"],
                sortino_ratio=best_doc["sortino_ratio"],
                backtest_start=best_doc["backtest_start"],
                backtest_end=best_doc["backtest_end"],
                timestamp=best_doc.get("timestamp"),
                status=best_doc["status"],
            )

            return ParameterOptimizationResult(
                strategy_id=strategy_id,
                best_parameters={},  # Would need to retrieve from strategy doc
                best_performance=best_performance,
                optimization_history=[best_performance],
                total_combinations_tested=1,
                optimization_time_seconds=0,
                timestamp=datetime.now(),
            )

        except Exception as e:
            logger.error("Error getting best existing performance: {str(e)}")
            raise

    def get_strategy_performance_summary(self, strategy_id: str) -> dict[str, Any]:
        """Get comprehensive performance summary for strategy"""
        try:
            # Get all performance records
            query = (
                self.firestore_client.collection("strategy_performance")
                .where("strategy_id", "==", strategy_id)
                .where("status", "==", "completed")
                .order_by("sharpe_ratio", direction=firestore.Query.DESCENDING)
            )

            performances = [doc.to_dict() for doc in query.stream()]

            if not performances:
                return {"strategy_id": strategy_id, "status": "no_data"}

            # Calculate summary statistics
            sharpe_ratios = [p["sharpe_ratio"] for p in performances]
            annual_returns = [p["annual_return"] for p in performances]

            summary = {
                "strategy_id": strategy_id,
                "total_backtests": len(performances),
                "best_sharpe_ratio": max(sharpe_ratios),
                "avg_sharpe_ratio": np.mean(sharpe_ratios),
                "best_annual_return": max(annual_returns),
                "avg_annual_return": np.mean(annual_returns),
                "performance_consistency": np.std(sharpe_ratios),
                "last_optimization": performances[0].get("timestamp"),
                "meets_threshold": max(sharpe_ratios) >= self.min_sharpe_ratio,
                "best_performance": performances[0],  # Best by Sharpe ratio
            }

            return summary

        except Exception as e:
            logger.error("Error getting strategy performance summary: {str(e)}")
            return {"strategy_id": strategy_id, "status": "error", "error": str(e)}

    def evolve_parameters_if_underperforming(self, strategy_id: str) -> bool:
        """
        Evolve parameters if strategy is underperforming

        Returns:
            True if evolution was triggered, False otherwise
        """
        try:
            summary = self.get_strategy_performance_summary(strategy_id)

            if summary.get("status") in ["no_data", "error"]:
                return False

            # Check if evolution is needed
            needs_evolution = (
                summary["best_sharpe_ratio"] < self.min_sharpe_ratio
                or summary["performance_consistency"] > 0.5  # High variance
            )

            if needs_evolution:
                logger.info(
                    "Strategy {strategy_id} needs parameter evolution. Best Sharpe: {summary['best_sharpe_ratio']:.2f}"
                )

                # Generate new parameter combinations around best performers
                # This would be implemented based on genetic algorithm or similar
                # For now, we'll mark for manual review

                evolution_doc = {
                    "strategy_id": strategy_id,
                    "reason": "underperforming",
                    "current_best_sharpe": summary["best_sharpe_ratio"],
                    "threshold": self.min_sharpe_ratio,
                    "status": "evolution_needed",
                    "timestamp": firestore.SERVER_TIMESTAMP,
                }

                self.firestore_client.collection("parameter_evolution").add(
                    evolution_doc
                )

                return True

            return False

        except Exception as e:
            logger.error(
                "Error checking evolution for strategy {strategy_id}: {str(e)}"
            )
            return False


# Health check and statistics functions
def health_check() -> dict[str, Any]:
    """Health check for Strategy Parameter Engine"""
    try:
        return {
            "status": "healthy",
            "module": "parameter_engine",
            "features": [
                "Parameter Optimization",
                "Performance Tracking",
                "Parameter Memory",
                "Auto-tuning",
                "Batch Processing",
            ],
            "dependencies": {
                "firestore": True,
                "bigquery": True,
                "concurrent_futures": True,
            },
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat(),
        }


def get_statistics() -> dict[str, Any]:
    """Get Strategy Parameter Engine statistics"""
    return {
        "module": "parameter_engine",
        "features": [
            "Parameter Optimization",
            "Performance Tracking",
            "Parameter Memory",
            "Auto-tuning",
            "Batch Processing",
        ],
        "optimization_methods": ["grid_search", "random_search", "bayesian"],
        "performance_metrics": [
            "sharpe_ratio",
            "annual_return",
            "max_drawdown",
            "win_rate",
        ],
        "timestamp": datetime.now().isoformat(),
    }


# CLI interface
if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        if sys.argv[1] == "--health":
            print(json.dumps(health_check(), indent=2))
        elif sys.argv[1] == "--stats":
            print(json.dumps(get_statistics(), indent=2))
        elif sys.argv[1] == "--test":
            print("ðŸ§ª Testing Strategy Parameter Engine...")
            result = health_check()
            print("Status: {result['status']}")
        else:
            print("Usage: python parameter_engine.py [--health|--stats|--test]")
    else:
        print("Strategy Parameter Engine - Use --help for usage")
